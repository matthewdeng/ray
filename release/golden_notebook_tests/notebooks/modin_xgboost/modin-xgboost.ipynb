{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150602c5",
   "metadata": {},
   "source": [
    "# XGBoost-Ray with Modin "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70840fa5",
   "metadata": {},
   "source": [
    "This notebook includes an example workflow using [XGBoost-Ray](https://docs.ray.io/en/latest/xgboost-ray.html) and [Modin](https://modin.readthedocs.io/en/latest/) for distributed model training and prediction.\n",
    "\n",
    "Please ensure you are running this notebook in a fresh Conda enviroment/virtualenv to avoid any package issues. See the included README for an example on how to set up a new `modin-xgboost-env-kernel` that this notebook can use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e2d21",
   "metadata": {},
   "source": [
    "## Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117ac35",
   "metadata": {},
   "source": [
    "First, we'll install the required dependencies and import them locally to verify that our local environment is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecf563",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[tune]\" \"xgboost_ray[default]\" modin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import modin.pandas as pd\n",
    "import ray\n",
    "\n",
    "from modin.experimental.sklearn.model_selection import train_test_split\n",
    "from ray import tune\n",
    "\n",
    "import xgboost_ray\n",
    "from xgboost_ray import RayDMatrix, RayParams, train, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52781f88",
   "metadata": {},
   "source": [
    "## Cluster Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc9901",
   "metadata": {},
   "source": [
    "Next, we'll set up our Ray Cluster. The `modin-xgboost.yaml` provided can be used to configure an AWS cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install boto3\n",
    "! ray up modin-xgboost.yaml -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a61ed",
   "metadata": {},
   "source": [
    "Now, let's connect our Python script to this newly deployed Ray cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52d3cc",
   "metadata": {},
   "source": [
    "### Connecting to the Ray cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f6696",
   "metadata": {},
   "source": [
    "In another terminal, run the following to set up port forwarding:\n",
    "\n",
    "```\n",
    "ray attach modin-xgboost.yaml -p 10001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfa9dc",
   "metadata": {},
   "source": [
    "You can then connect directly to port `10001` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(address=\"ray://localhost:10001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c936e5",
   "metadata": {},
   "source": [
    "### Alternative Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8cd4ac",
   "metadata": {},
   "source": [
    "**Note:** This does not work with Ray 1.6.0 but is fixed in latest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = ! ray get-head-ip modin-xgboost.yaml | tail -n 1\n",
    "print(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd5cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ray_address = f\"ray://{address[0]}:10001\"\n",
    "print(ray_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ray --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d09e67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ray.init(address=ray_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5dc603",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44bf7bf",
   "metadata": {},
   "source": [
    "We will use the [HIGGS dataset from the UCI Machine Learning dataset repository](https://archive.ics.uci.edu/ml/datasets/HIGGS). The HIGGS dataset consists of 11,000,000 samples and 28 attributes, which is a large enough size to show the benefits of distributed computation.\n",
    "\n",
    "We set the Dask scheduler to ray_dask_get to use Dask on Ray backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4827b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "# Test dataset with only 10,000 records.\n",
    "FILE_URL = \"https://ray-ci-higgs.s3.us-west-2.amazonaws.com/simpleHIGGS.csv\"\n",
    "\n",
    "# Uncomment this to run on the actual dataset. This may take a couple of minutes to run.\n",
    "#FILE_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\"\n",
    "\n",
    "colnames = [LABEL_COLUMN] + [\"feature-%02d\" % i for i in range(1, 29)]\n",
    "\n",
    "load_data_start_time = time.time()\n",
    "\n",
    "# Force read_csv to be executed on the Ray server.\n",
    "df = ray.get(ray.remote(pd.read_csv).remote(FILE_URL, names=colnames))\n",
    "\n",
    "load_data_end_time = time.time()\n",
    "load_data_duration = load_data_end_time - load_data_start_time\n",
    "\n",
    "print(f\"Dataset loaded in {load_data_duration} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a549576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation.\n",
    "df_train, df_validation = train_test_split(df)\n",
    "\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e617247",
   "metadata": {},
   "source": [
    "## Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ecf2f",
   "metadata": {},
   "source": [
    "The `train_xgboost` function contains all of the logic necessary for training using XGBoost-Ray.\n",
    "\n",
    "Distributed training can not only speed up the process, but also allow you to use datasets that are to large to fit in memory of a single node. With distributed training, the dataset is sharded across different actors running on separate nodes. Those actors communicate with each other to create the final model.\n",
    "\n",
    "First, the dataframes are wrapped in `RayDMatrix` objects, which handle data sharding across the cluster. Then, the `train` function is called. The evaluation scores will be saved to `evals_result` dictionary. The function returns a tuple of the trained model (booster) and the evaluation scores.\n",
    "\n",
    "The `ray_params` variable expects a `RayParams` object that contains Ray-specific settings, such as the number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7869073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(config, train_df, test_df, target_column, ray_params):\n",
    "    train_set = RayDMatrix(train_df, target_column)\n",
    "    test_set = RayDMatrix(test_df, target_column)\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    train_start_time = time.time()\n",
    "    \n",
    "    # Train the classifier\n",
    "    bst = train(\n",
    "        params=config,\n",
    "        dtrain=train_set,\n",
    "        evals=[(test_set, \"eval\")],\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=False,\n",
    "        num_boost_round=100,\n",
    "        ray_params=ray_params)\n",
    "    \n",
    "    train_end_time = time.time()\n",
    "    train_duration = train_end_time - train_start_time\n",
    "    print(f\"Total time taken: {train_duration} seconds.\")\n",
    "\n",
    "    model_path = \"model.xgb\"\n",
    "    bst.save_model(model_path)\n",
    "    print(\"Final validation error: {:.4f}\".format(\n",
    "        evals_result[\"eval\"][\"error\"][-1]))\n",
    "\n",
    "    return bst, evals_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab30d2",
   "metadata": {},
   "source": [
    "We can now pass our Modin dataframes and run the function. We will use `RayParams` to specify that our model is to be trained on 4 actors.\n",
    "\n",
    "The dataset has to be downloaded onto the cluster, which may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4a9b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard XGBoost config for classification\n",
    "config = {\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "}\n",
    "\n",
    "bst, evals_result = train_xgboost(config, df_train, df_validation, LABEL_COLUMN, RayParams(cpus_per_actor=8, num_actors=4))\n",
    "print(evals_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777738a",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64df5b5",
   "metadata": {},
   "source": [
    "With the model trained, we can now predict on unseen data. For the purposes of this example, we will use the same dataset for prediction as for training.\n",
    "\n",
    "Since prediction is naively parallelizable, distributing it over multiple actors can measurably reduce the amount of time needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = RayDMatrix(df, ignore=[LABEL_COLUMN, \"partition\"])\n",
    "results = predict(\n",
    "    bst,\n",
    "    inference_df,\n",
    "    ray_params=RayParams(cpus_per_actor=2, num_actors=16))\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modin-xgboost-env-kernel",
   "language": "python",
   "name": "modin-xgboost-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
